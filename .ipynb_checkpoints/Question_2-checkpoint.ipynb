{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "####################### GENERAL AUXILIARY FUNCTIONS #######################\n",
    "## The following structure helps us to have functions with multiple outputs\n",
    "### credit: https://stat.ethz.ch/pipermail/r-help/2004-June/053343.html\n",
    "list <- structure(NA,class=\"result\")\n",
    "\"[<-.result\" <- function(x,...,value) {\n",
    "  args <- as.list(match.call())\n",
    "  args <- args[-c(1:2,length(args))]\n",
    "  length(value) <- length(args)\n",
    "  for(i in seq(along=args)) {\n",
    "    a <- args[[i]]\n",
    "    if(!missing(a)) eval.parent(substitute(a <- v,list(a=a,v=value[[i]])))\n",
    "  }\n",
    "  x\n",
    "}\n",
    "\n",
    "# reading the data\n",
    "read.data <- function(file.name, scaling=FALSE) {\n",
    "  data <- read.csv(file=file.name,head=TRUE,sep=\",\")\n",
    "  data <- data[complete.cases(data),] # removes rows with NA values\n",
    "  D <- ncol(data)\n",
    "  x = data[,-D]\n",
    "  y = data[,D]\n",
    "  if (isTRUE(scaling)) {\n",
    "    x = scale(x)\n",
    "    y = scale(y)\n",
    "  }\n",
    "  return (list('x' = x, 'y' = y))\n",
    "}\n",
    "set.seed(1243)\n",
    "error.rate <- function(Y1, T1){\n",
    "  if (length(Y1)!=length(T1)){\n",
    "    stop('error.rate: size of true lables and predicted labels mismatch')\n",
    "  }\n",
    "  return (sum(T1!=Y1)/length(T1))\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "####################### NEURAL NET ####################### \n",
    "## the activation function (tanh here)\n",
    "h <- function(z, a=1) { #activation function (sigmoid here)\n",
    "  return ((exp(z)-a^(-z))/(exp(z)+exp(-z)))\n",
    "}\n",
    "## the derivitive of the activation function (tanh here)\n",
    "h.d <- function(z) {\n",
    "  return (1-(h(z))^2)\n",
    "}\n",
    "## Class Probabilities\n",
    "class.prob <- function(X, W1, W2, b1, b2){\n",
    "  a2 <- h(sweep(W1 %*% X, 1, b1,'+' ))\n",
    "  a3 <- h(sweep(W2 %*% a2, 1, b2,'+' ))\n",
    "  return (a3)\n",
    "}\n",
    "## prediction\n",
    "nn.predict <- function(X, W1, W2, b1, b2, threshold=0.5){\n",
    "  return (ifelse(class.prob(X, W1, W2, b1, b2)>=threshold, 1, -1))\n",
    "}\n",
    "## feedforward step\n",
    "feedforward <- function(Xi, Ti, W1, b1, W2, b2){\n",
    "  ### 1st (input) layer \n",
    "  a1 <- Xi\n",
    "  y <- Ti\n",
    "  ### 2nd (hidden) layer\n",
    "  z2 <- W1 %*% a1 + b1\n",
    "  a2 <- h(z2)        \n",
    "  ### 3rd (output) layer\n",
    "  z3 <- W2 %*% a2 + b2\n",
    "  a3 <- h(z3)  \n",
    "  return(list(a1, a2, a3, y, z2, z3))\n",
    "}\n",
    "## backpropagation step\n",
    "backpropagation <- function(Ti, W2, z2, z3, a3){\n",
    "  ### 3rd (output) layer\n",
    "  d3 <- -(Ti-a3) * h.d(z3)\n",
    "  ### 2nd (hidden) layer\n",
    "  d2 <-  t(W2)%*%d3  * h.d (z2)\n",
    "  return(list(d2,d3))\n",
    "}\n",
    "## NN build function\n",
    "nn.build <- function(K, X1, T1, epoch.max=50, eta = 0.1, lambda = 0.01){\n",
    "  # initialization\n",
    "#   if (plotting) {error.rec <- matrix(NA,nrow=epoch.max, ncol=1)}\n",
    "  D <- nrow(X1)\n",
    "  if (D!=2) {stop('nn.predict: This simple version only accepts two dimensional data.')}\n",
    "  N <- ncol(X1)\n",
    "\n",
    "  W1 <- matrix(rnorm(D*K, sd=0.5), nrow=K, ncol=D)\n",
    "  b1 <- matrix(rnorm(1*K), nrow=K, ncol=1)\n",
    "  W2 <- matrix(rnorm(K*1, sd=0.5), nrow=1, ncol=K)\n",
    "  b2 <- matrix(rnorm(1*1), nrow=1, ncol=1)\n",
    "\n",
    "  for (epoch in 1:epoch.max){   \n",
    "    ## delta vectors/matrices initialization\n",
    "    W1.d <- W1 *0\n",
    "    b1.d <- b1 *0\n",
    "    W2.d <- W2 *0\n",
    "    b2.d <- b2 *0\n",
    "\n",
    "    for (i in 1:N){\n",
    "      ## Feedforward:\n",
    "      list[a1, a2, a3, y, z2, z3] <- feedforward(X1[,i], T1[i], W1, b1, W2, b2)          \n",
    "      ## Backpropagation:\n",
    "      list[d2, d3] <- backpropagation(T1[i], W2, z2, z3, a3)\n",
    "      ## calculate the delta values\n",
    "      ### 1st layer\n",
    "      W1.d <- W1.d + d2 %*% t(a1)\n",
    "      b1.d <- b1.d + d2\n",
    "      ### 2nd layer\n",
    "      W2.d <- W2.d + d3 %*% t(a2)\n",
    "      b2.d <- b2.d + d3\n",
    "    }\n",
    "    ## update weight vectors and matrices\n",
    "    W1 <- W1 - eta * (W1.d/N + lambda*W1)\n",
    "    b1 <- b1 - eta * (b1.d/N)\n",
    "    W2 <- W2 - eta * (W2.d/N + lambda*W2)\n",
    "    b2 <- b2 - eta * (b2.d/N)\n",
    "    ## record the errors\n",
    "#     if (plotting){error.rec[epoch]<- error.rate(nn.predict(X1, W1, W2, b1, b2), T1)}\n",
    "  }\n",
    "#   plot(error.rec, xlab = 'epoch', ylab = 'error', main = 'Neural Net')\n",
    "  return(list(W1, W2, b1, b2))\n",
    "}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "####################### Assignment 3.B #######################\n",
    "# Read the datasets\n",
    "set.seed(1234)          # set random seed\n",
    "library(ggplot2)        # load libraries\n",
    "list[X1,T1] <- read.data('Task2B_train.csv') # read training data\n",
    "T1[T1==0] <- -1         # convert 0 labels to -1 \n",
    "list[X2,T2] <- read.data('Task2B_test.csv') # read test data\n",
    "T2[T2==0] <- -1         # convert 0 labels to -1 \n",
    "\n",
    "ctr = 0\n",
    "\n",
    "\n",
    "# Build a perceptron and plot its train error curve\n",
    "# W<-perceptron.build(X1, T1, tau.max = 1000, plotting = TRUE) # Run this a few times until you are happy with the result\n",
    "\n",
    "\n",
    "\n",
    "#? Build a number of Neural Networks with different number of units in the hidden layer (TO BE COMPLETE)\n",
    "error <- data.frame('ctr' = 1:20 , 'K' = seq(5,100,5) , 'error_0.01' = rep(0,20),'error_0.09' = rep(0,20))\n",
    "for (k in seq(5, 100, 5)) {\n",
    "  list[W1_0.01, W2_0.01, b1_0.01, b2_0.01]<- nn.build(k, t(as.matrix(X1)), T1, epoch.max=1000, eta = 0.01, lambda = 0.01)\n",
    "  list[W1_0.09, W2_0.09, b1_0.09, b2_0.09]<- nn.build(k, t(as.matrix(X1)), T1, epoch.max=1000, eta = 0.09, lambda = 0.01)\n",
    "  \n",
    "    #? Evaluate the model (TO BE COMPLETE)\n",
    "  pred_nn_0.01 = nn.predict(t(as.matrix(X2)),W1_0.01,W2_0.01,b1_0.01,b2_0.01)\n",
    "  \n",
    "  pred_nn_0.09 = nn.predict(t(as.matrix(X2)),W1_0.09,W2_0.09,b1_0.09,b2_0.09)\n",
    "  #? Record the test errors for plotting purposes (TO BE COMPLETE)\n",
    "error[ctr,2] = k\n",
    "error[ctr,3] = error.rate(pred_nn_0.01 , T2)\n",
    "error[ctr,4] = error.rate(pred_nn_0.09, T2)\n",
    "ctr = ctr + 1\n",
    "    \n",
    "}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#? Plot the test error versus number of units i.e., k  (TO BE COMPLETE)\n",
    "\n",
    "ggplot(data = error) + geom_line(aes(K,error_0.01,color = \"eta : 0.01\")) + geom_line(aes(K,error_0.09,color = \"eta : 0.09\")) + theme_minimal()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#? save all the plots and attach them to your report. Also, add the obtained test error as a table in your report. Finally, answer the questions and explain your findings.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#? Find the k with the lowest test error (TO BE COMPLETE)\n",
    "error(which.min(error$error_0.09),] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#? Plot Decision Boundary for NN with the lowest test error (TO BE COMPLETE)\n",
    "list[W1,W2,b1,b2] <- nn.build(k,t(as.matrix(X1),T,plotting = FALSE, epoch.max = 1000, eta = 0.01, lambda = 0.01)\n",
    "pred_nn_0.09 <- nn.predict(t(as.matrix(X2), W1, W2, b1, b2, threshold = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the testing data with different symbols for each class (real labels). Then color each point based on its predicted label.\n",
    "test_d <- as.data.frame(cbind(X2, t(pred_nn_0.09)))\n",
    "colnames(test_d) <- c(\"X1\", \"X2\" , \"Label\")\n",
    "test_d$Label = as.factor(test_d$Label)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ggplot(data = test_d, aes(x = X1, y = X2)) + geom_point(aes(color = Label)) + ggtitle(\"Test data with best model\") + theme_minimal()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
